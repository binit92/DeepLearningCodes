################################ Required Files and TEsts ####################################################################
All required files are present :+1:
+ You can include /data directory too in the repository, so that anyone can run it. It has small size 
+ It is recommended to export your conda environment into environment.yaml file. command **conda env export -f environment.yaml**, so that you can recreate your conda environment later.
+ All these things will help you when you will revisit your code at Github in future. Check this [link](https://udacity.github.io/git-styleguide/) for best practice.

All required files are present :+1:
+ It is asked to submit .html version of notebook too so that anyone can easily open the notebook.
+ It is recommended to export your conda environment into environment.yaml file. command **conda env export -f environment.yaml**, so that you can recreate your conda environment later.
+ When submitting this code to VCS (say Github), make sure to remove unnecessary files e.g. save.data files. It will reduce project size.

Please include both a Jupyter notebook file and an HTML or PDF export of your project with all future Udacity submissions. I will be able to review the majority of your code, but without the HTML file, I won't be able to see what your project produced as output in your test environment.


Well Done ! :+1:  Donald Knuth (a famous computer science pioneer) once famously said 
> “Beware of bugs in the above code; I have only proved it correct, not tried it.”
################################## PreProcessing #######################################################################
Good work :+1:
+ In particular, using a python set function to ensure each entry in the vocab list is unique is an excellent technique to remove duplicates from a list, and something that you will find yourself doing as part of almost any dataset-preparation work.
+  when creating lookup tables it can also be helpful to index words by the frequency each word occurs in the text. The python Counter function (part of the collections library) is a convenient way to get the information needed for that approach. 
https://pymotw.com/2/collections/counter.html

:+1: 
+ when creating lookup tables it is helpful to index words by the frequency each word occurs in the text. The python Counter function (part of the collections library) is a convenient way to get the information needed for that approach. 
```
 word_counts = Counter(text)
    sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)
    vocab_to_int = dict()
    int_to_vocab = dict()
    
    for i, word in enumerate(sorted_words):
        vocab_to_int[word] = i
        int_to_vocab[i] = word
    return vocab_to_int, int_to_vocab
```	
##########################################################################################################################3

:+1:
+ Converting each punctuation into explicit token is very handy when working with RNNs.
+ Do read up this [link](https://datascience.stackexchange.com/a/11421) to understand what other pre-processing steps are carried out before feeding text data to RNNs.
+ An alternate implementation would be : 
```
 return {
        '.'  : '||period||',
        ','  : '||comma||',
        '"'  : '||quotationmark||',
        ';'  : '||semicolon||',
        '!'  : '||exclamationmark||',
        '?'  : '||questionmark||',
        '('  : '||leftparentheses',
        ')'  : '||rightparentheses',
        '--' : '||doubledash||',
        '\n' : '||return||'
   }
```  


############################### Build the Neural Network ###########################################################################
:clap:
+ Good Job running it on GPU 
+ I like the fact that you named each tf tensor, and not just the one required by the rubric. This is very helpful in debugging, and something I always do for every tf placeholder.
+ I recommend using named-parameter-passing for all variables where the meaning isn’t obvious. For example, using  `shape=[None, None]` when passing the default shape to the placeholder function.
+ If you keep learning rate as integer (tf.int32), the learning rate (0.001) becomes 0. so using tf.float32 is a right choice here.
+ `input` as a variable name in python . ref : https://stackoverflow.com/a/20670757


####################################################################################################################################

Awesome :blush:
+ You've correctly stacked BasicLSTMCell into a MultiRNNCell, creating the core of your RNN.
+ For this project, 1 or 2 number of lstm cells provided the best output.

+ Although it is not required, bit You could try implementing  DropoutWrapper too . 
https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper
+ An alternate implementation would be:
```
 num =2
 lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)
 drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob =0.8)
 cell = tf.contrib.rnn.MultiRNNCell([drop] * num)
```
########################################################################################################################################

Good Work :+1: 
+ Alternatively, you could also use 
`return tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)`
+ You can see this [video](https://www.youtube.com/watch?v=Eku_pbZ3-Mw) on embedding
	
Correct use of tf.nn.embedding_lookup. Embedding are necessary because Neural Nets need numbers to crunch instead of words.
However, you used tf.get_variable which used a glorot_uniform_initializer, you may try different initializer
(e.g. tf.random_normal or tf.truncated_normal distribution, with a small standard deviation can be very good.)	

	
Good! A uniform distribution is appropriate here, and your code is straight-forward and meets spec. It appears, though, that you might be somewhat confused about word embeddings.
An embedding is a multi-dimensional vector used to represent words in a corpus, or dictionary, such that words with similar meanings end up at similar "locations" in the multi-dimensional space
covered by the embedding vector. Typical embedding vectors have 300 elements, and each vector thus represents a "point" in a 300-dimensional space. So, there are, indeed, multiple dimensions.


########################################################################################################################################

`tf.nn.dynamic_rnn` used correctly. Tensorflow provides 2 RNN components:
+ tf.nn.rnn is used for applications where there is a fixed length unrolled RNN.
+ tf.nn.dynamic_rnn uses a tf.While loop to dynamically construct the graph at runtime.



################################## Build RNN ##################################################################################33


:clap:
+ When activation function is specified as 'None', it takes linear activation. I recommend you to add activation parameter as "activation=None". It makes implementation more readable. 
+ Reference on [activation](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)
+ You could also use **tf.layers.dense** here, It is essentially the same [thing](https://stackoverflow.com/questions/44912297/are-tf-layers-dense-and-tf-contrib-layers-fully-connected-interchangeable11)

+ Checkout difference between tf-truncated-normal-and-tf-random-normal 
https://stackoverflow.com/questions/41704484/what-is-difference-between-tf-truncated-normal-and-tf-random-normal

https://www.youtube.com/watch?v=yCC09vCHzF8&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&index=10
the examples between 22:30 - 31:15


########################################################  Batching #####################################################################
:clap:
+ Your efforts reveals that you understand batching really well .
+ Good use of numpy, I encourage you to use it more 
+ You code is readable and I encourage you to write comments.
+ Please add  a debug statement showing that 
`print(get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2) )`

+ Alternatively, it could also be implemented as : 
```
 number_of_batches = (len(int_text) - 1) // (batch_size * seq_length)
 input_value = np.array(int_text[: number_of_batches * batch_size * seq_length])
 target_value = np.array(int_text[1: number_of_batches * batch_size * seq_length + 1])
 target_value[-1] = 0
 input_batches = np.split(input_value.reshape(batch_size, -1), number_of_batches, 1)
 target_batches = np.split(target_value.reshape(batch_size, -1), number_of_batches, 1)
 batches = list(zip(input_batches, target_batches))
 return np.array(batches)
```
+ or,
```
	need = batch_size*seq_length
    need = need*(len(int_text)//need)
    data = np.array(int_text)
    data = data[:need]
    num = len(data)//(batch_size*seq_length)
    inputArr = data[0:len(data)]
    targetArr = np.hstack([data[1:len(data)], data[0:1]])
    inputBatch = np.reshape(inputArr, (batch_size, -1))
    targetBatch = np.reshape(targetArr, (batch_size, -1))
    inputBatches = np.split(inputBatch, num, axis=1)
    targetBatches = np.split(targetBatch, num, axis=1)
    AllDoneBatches = list(zip(inputBatches, targetBatches))
    npFinalBatches = np.array(AllDoneBatches)
    return npFinalBatches
```

###################################################################### Hyperparameter ###############################################################3	

:rocket:
+ Enough epochs to get near a minimum in the training loss. 
+ Batch size is large enough to train efficiently  
+ Size of the RNN cells is large enough to fit the data well 
+ Sequence length is about the size of the length of sentences we want to generate 
+ Size of embedding is in the range of [200-300] 
+ Learning rate seems good based on other hyper parameter 

> Your efforts shows that you have really have executed it again and again to get an optimized value :fire:


:rocket:

+ Batch size is large enough to train efficiently  :+1:
+ Size of the RNN cells is large enough to fit the data well :+1: 
+ Learning rate seems good based on other hyper parameter :+1:  

:triangular_flag_on_post:
+ Enough epochs to get near a minimum in the training loss. But it is too high and doing extra computation unnecesarilly . Normally it would work with epochs size upto 500
+ Sequence length should be  about the size of the length of sentences we want to generate [10 to 30]
+ Size of embedding works but the good range is [200-300]

> Your efforts shows that you have really have executed it again and again to get an optimized value :fire:

############################################################################ Overall loss ############################################################

:+1:
Note that getting the loss below 1.0 is a key objective requirement, but not the only measure of success on this project.
 While num_epochs and learning_rate are key to getting the necessary loss value, the other factors affect the quality of the generated script 
 and are just as important as the loss value in determining overall success on this project.

######################################################################################################################################################

return loaded_graph.get_tensor_by_name('input:0'), \
           loaded_graph.get_tensor_by_name('initial_state:0'), \
           loaded_graph.get_tensor_by_name('final_state:0'), \
           loaded_graph.get_tensor_by_name('probs:0')

		   

############################################################################## Pick Word ####################################################################

+ This works, but you should add in some randomness that takes in probabilities as a parameter instead of always choosing the highest probability word.
+ Your current implementation will always choose the highest probability word, and it is preferable so the predictions don't fall into a loop of the same words (or sequences). 

:triangular_flag_on_post: 

```
vocab_list = ["hello", "how", "is"]
probabilities = [0.3,0.5,0.2]
np.random.choice(vocab_list, p=probabilities)
```
Now you can see in the above that the word 
+ **how** is likely to appear by 0.5 probability, 
+ **hello** is likely to appear by 0.3 probability, 
+ **is** is likely to appear by 0.2 probability.
You are encouraged to use slight randomness when choosing the next word. If you don’t, the predictions can fall into a loop of the same words.
You could also use np.random.choice() here https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html

```
idx = np.random.choice(len(probabilities), p=probabilities)
return int_to_vocab[idx]
```

	
######################################################################################################################################################
Congratulations :tada: 
+ Your submission reveals that you have made a great effort in finishing this project. It is an important milestone in learning about RNNs
+ Very good hyperparameters and loss . It is great that you have got everything right in first review :+1:
+ I wish you all the best for next adventures :rocket:
+ Nice Read: (Colah's Blog)   http://colah.github.io/posts/2015-08-Understanding-LSTMs/
+ Nice Read: (Andrej Karpathy) : http://karpathy.github.io/2015/05/21/rnn-effectiveness/
+ Nice Read: (Rohan Kapur) https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b

Keep up the good work :+1:  Stay Udacious :udacious:

:fire: 
+ You have made a **real good effort** in finishing this project. This is an important mile stone in  deep-learning. 
+ With suggested changes in hyperparameters, I am sure you will be able to get a loss less than 1.0 :checkered_flag: 
+ Very good hyperparameters and loss . It is great that you have got everything right in first review :+1:
+ In case of doubts, You can find pointers from mentors and `discussions.udacity.com` and `knowledge.udacity.com`
+ Nice Read: (Colah's Blog)   http://colah.github.io/posts/2015-08-Understanding-LSTMs/
+ Nice Read: (Andrej Karpathy) : http://karpathy.github.io/2015/05/21/rnn-effectiveness/
+ Nice Read: (Rohan Kapur) https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b

Keep up the good work :+1:  Stay Udacious :udacious:

Excellent  :rocket:
+ Your submission reveals that you have made a **great effort** in finishing this project. It is an important milestone in learning about RNNs
+ Very good hyperparameters and loss . It is great that you have got everything right in first review :+1:
+ Just add randomness in picking next word and you are good to go ! :checkered_flag:
+ Nice Read: (Colah's Blog)   http://colah.github.io/posts/2015-08-Understanding-LSTMs/
+ Nice Read: (Andrej Karpathy) : http://karpathy.github.io/2015/05/21/rnn-effectiveness/
+ Nice Read: (Rohan Kapur) https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b

Keep up the good work !  :+1: Stay Udacious :udacious:
 